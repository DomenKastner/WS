##WebScraping##

**My WS playground an useful sites about WS**

**Rules**

-1-Check robots.txt--

The scraping rules of the websites can be found in the robots.txt file.
You can find it by writing robots.txt after the main domain, e.g www.website_to_scrape.com/robots.txt.
These rules identify which parts of the websites are not allowed to be automatically extracted or how frequently a bot is allowed to request a page.
Most people don’t care about it, but try to be respectful and at least look at the rules even if you don’t plan to follow them.


**Useful Links**

[Some article](https://www.analyticsvidhya.com/blog/2015/10/beginner-guide-web-scraping-beautiful-soup-python/)

[a few tips and tricks](https://hackernoon.com/web-scraping-tutorial-with-python-tips-and-tricks-db070e70e071)

[BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

[Ninja-mambers only](https://www.smartninja.org/student/course/5644317255270400/lesson/4855289266307072) 

[WebScraper.io](http://webscraper.io/)